import requests
import csv
import time
import json
import os
from datetime import datetime
from typing import List, Dict, Any

class XeetLeaderboardParser:
    def __init__(self, tournament_type="leagues"):
        """
        Initialize parser for specific tournament type
        tournament_type: "leagues" or "signals"
        """
        self.tournament_type = tournament_type
        
        # API endpoints for different tournaments
        self.tournament_configs = {
            "leagues": {
                "url": "https://www.xeet.ai/api/tournaments/5ea420b7-17c1-4a9d-9501-0fcaa60387f9/leaderboard",
                "total_pages": 431,
                "csv_file": "xeet_leagues_stats.csv",
                "avatars_file": "xeet_leagues_avatars.csv",
                "metadata_file": "xeet_leagues_metadata.json"
            },
            "signals": {
                "url": "https://www.xeet.ai/api/tournaments/xeet-tournament-1/leaderboard",
                "total_pages": 360,  # Based on the API response showing 360 pages
                "csv_file": "xeet_signals_stats.csv",
                "avatars_file": "xeet_signals_avatars.csv",
                "metadata_file": "xeet_signals_metadata.json"
            }
        }
        
        # Set configuration based on tournament type
        config = self.tournament_configs[tournament_type]
        self.base_url = config["url"]
        self.csv_file = config["csv_file"]
        self.avatars_file = config["avatars_file"]
        self.metadata_file = config["metadata_file"]
        self.total_pages = config["total_pages"]
        self.limit = 20
        self.delay = 0.5
        
        self.session = requests.Session()
        # Add headers to mimic browser
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        })
        
    def fetch_page(self, page: int) -> Dict[str, Any]:
        """ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ñ Ð¾Ð´Ð½Ð¾Ð¹ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ API"""
        params = {
            'page': page,
            'limit': self.limit
        }
        
        try:
            response = self.session.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐµ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ {page}: {e}")
            return None
        except json.JSONDecodeError as e:
            print(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° JSON Ð´Ð»Ñ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ {page}: {e}")
            return None
    
    def get_last_updated_from_api(self) -> str:
        """ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ Ð´Ð°Ñ‚Ñƒ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð¸Ð· API (Ð¿ÐµÑ€Ð²Ð°Ñ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°)"""
        try:
            response = self.session.get(self.base_url, params={'page': 1, 'limit': 1}, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            # lastUpdated Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑÑ Ð² ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð·Ð°Ð¿Ð¸ÑÐ¸, Ð±ÐµÑ€ÐµÐ¼ Ð¸Ð· Ð¿ÐµÑ€Ð²Ð¾Ð¹
            if 'data' in data and len(data['data']) > 0:
                return data['data'][0].get('lastUpdated', '')
            return ''
        except Exception as e:
            print(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ð¸ Ð´Ð°Ñ‚Ñ‹ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ: {e}")
            return ''
    
    def load_metadata(self) -> Dict[str, Any]:
        """Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· Ñ„Ð°Ð¹Ð»Ð°"""
        if os.path.exists(self.metadata_file):
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐµ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…: {e}")
        return {}
    
    def save_metadata(self, metadata: Dict[str, Any]):
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² Ñ„Ð°Ð¹Ð»"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ð¸ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…: {e}")
    
    def check_if_update_needed(self) -> bool:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚, Ð½ÑƒÐ¶Ð½Ð¾ Ð»Ð¸ Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÑ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ"""
        print("ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ…...")
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰ÑƒÑŽ Ð´Ð°Ñ‚Ñƒ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð¸Ð· API
        current_last_updated = self.get_last_updated_from_api()
        if not current_last_updated:
            print("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð´Ð°Ñ‚Ñƒ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð¸Ð· API. ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³...")
            return True
        
        print(f"Ð”Ð°Ñ‚Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð² API: {current_last_updated}")
        
        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
        metadata = self.load_metadata()
        saved_last_updated = metadata.get('lastUpdated', '')
        
        if not saved_last_updated:
            print("ÐœÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹. Ð¢Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³.")
            return True
        
        print(f"Ð”Ð°Ñ‚Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð² ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…: {saved_last_updated}")
        
        # Ð¡Ñ€Ð°Ð²Ð½Ð¸Ð²Ð°ÐµÐ¼ Ð´Ð°Ñ‚Ñ‹
        if current_last_updated == saved_last_updated:
            print("âœ… Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð°ÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹! ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð½Ðµ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ.")
            return False
        else:
            print("ðŸ”„ Ð”Ð°Ð½Ð½Ñ‹Ðµ ÑƒÑÑ‚Ð°Ñ€ÐµÐ»Ð¸. Ð¢Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ.")
            return True
    
    def extract_influencer_data(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ð½ÑƒÐ¶Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· Ð¾Ð±ÑŠÐµÐºÑ‚Ð° Ð¸Ð½Ñ„Ð»ÑŽÐµÐ½ÑÐµÑ€Ð°"""
        user = item.get('user', {})
        
        return {
            'rank': item.get('rank', 0),
            'username': user.get('username', ''),
            'followerCount': user.get('followerCount', 0),
            'score': round(item.get('score', 0), 2),
            'signalScore': round(item.get('signalScore', 0), 2),
            'noisePoints': round(item.get('noisePoints', 0), 2),
            'totalEngagement': item.get('totalEngagement', 0),
            'engagementRate': item.get('engagementRate', 0),
            'averageEngagementPerPost': item.get('averageEngagementPerPost', 0)
        }
    
    def extract_avatar_data(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð°Ð²Ð°Ñ‚Ð°Ñ€Ð° Ð¸Ð· Ð¾Ð±ÑŠÐµÐºÑ‚Ð° Ð¸Ð½Ñ„Ð»ÑŽÐµÐ½ÑÐµÑ€Ð°"""
        user = item.get('user', {})
        
        return {
            'username': user.get('username', ''),
            'avatar': user.get('avatar', ''),
            'name': user.get('name', '')
        }
    
    def parse_all_pages(self) -> tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """ÐŸÐ°Ñ€ÑÐ¸Ñ‚ Ð²ÑÐµ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÐºÐ¾Ñ€Ñ‚ÐµÐ¶ (Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ, Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð°Ð²Ð°Ñ‚Ð°Ñ€Ð¾Ð²)"""
        all_data = []
        all_avatars = []
        
        for page in range(1, self.total_pages + 1):
            print(f"ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ {page}/{self.total_pages}...")
            
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹
            page_data = self.fetch_page(page)
            
            if page_data is None:
                print(f"ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ {page} Ð¸Ð·-Ð·Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ¸")
                continue
            
            # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¸Ð½Ñ„Ð»ÑŽÐµÐ½ÑÐµÑ€Ð¾Ð²
            influencers = page_data.get('data', [])
            
            if not influencers:
                print(f"Ð¡Ñ‚Ñ€Ð°Ð½Ð¸Ñ†Ð° {page} Ð½Ðµ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ…")
                continue
            
            # ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¸Ð½Ñ„Ð»ÑŽÐµÐ½ÑÐµÑ€Ð°
            for influencer in influencers:
                extracted_data = self.extract_influencer_data(influencer)
                avatar_data = self.extract_avatar_data(influencer)
                all_data.append(extracted_data)
                all_avatars.append(avatar_data)
            
            print(f"Ð¡Ñ‚Ñ€Ð°Ð½Ð¸Ñ†Ð° {page}: Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¾ {len(influencers)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
            
            # Ð—Ð°Ð´ÐµÑ€Ð¶ÐºÐ° Ð¼ÐµÐ¶Ð´Ñƒ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°Ð¼Ð¸
            time.sleep(0.5)
        
        return all_data, all_avatars
    
    def save_to_csv(self, data: List[Dict[str, Any]], filename: str = "xeet_crypto_creators_stats.csv"):
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² CSV Ñ„Ð°Ð¹Ð»"""
        if not data:
            print("ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ")
            return
        
        fieldnames = [
            'rank', 'username', 'followerCount', 'score', 'signalScore',
            'noisePoints', 'totalEngagement', 'engagementRate', 'averageEngagementPerPost'
        ]
        
        try:
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(data)
            
            print(f"Ð”Ð°Ð½Ð½Ñ‹Ðµ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð² Ñ„Ð°Ð¹Ð» {filename}")
            print(f"Ð’ÑÐµÐ³Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹: {len(data)}")
            
        except Exception as e:
            print(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ð¸ Ñ„Ð°Ð¹Ð»Ð°: {e}")
    
    def save_avatars_to_csv(self, avatar_data: List[Dict[str, Any]], filename: str = "xeet_avatars.csv"):
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð°Ð²Ð°Ñ‚Ð°Ñ€Ð¾Ð² Ð² Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ð¹ CSV Ñ„Ð°Ð¹Ð»"""
        if not avatar_data:
            print("ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð°Ð²Ð°Ñ‚Ð°Ñ€Ð¾Ð² Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ")
            return
        
        fieldnames = ['username', 'avatar', 'name']
        
        try:
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(avatar_data)
            
            print(f"Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð°Ð²Ð°Ñ‚Ð°Ñ€Ð¾Ð² ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð² Ñ„Ð°Ð¹Ð» {filename}")
            print(f"Ð’ÑÐµÐ³Ð¾ Ð°Ð²Ð°Ñ‚Ð°Ñ€Ð¾Ð²: {len(avatar_data)}")
            
        except Exception as e:
            print(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ð¸ Ñ„Ð°Ð¹Ð»Ð° Ð°Ð²Ð°Ñ‚Ð°Ñ€Ð¾Ð²: {e}")
    
    def save_metadata_after_parsing(self, last_updated: str, total_records: int):
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ð¾ÑÐ»Ðµ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾Ð³Ð¾ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°"""
        metadata = {
            'lastUpdated': last_updated,
            'totalRecords': total_records,
            'parsedAt': datetime.now().isoformat(),
            'totalPages': self.total_pages
        }
        self.save_metadata(metadata)
        print(f"ÐœÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹: {last_updated}")
    
    def run(self):
        """ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð´Ð»Ñ Ð·Ð°Ð¿ÑƒÑÐºÐ° Ð¿Ð°Ñ€ÑÐµÑ€Ð°"""
        print(f"ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Ð»Ð¸Ð´ÐµÑ€Ð±Ð¾Ñ€Ð´Ð° Xeet.ai ({self.tournament_type.upper()})...")
        print(f"Ð’ÑÐµÐ³Ð¾ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ† Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸: {self.total_pages}")
        print("-" * 50)
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ð½ÑƒÐ¶Ð½Ð¾ Ð»Ð¸ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ
        if not self.check_if_update_needed():
            print("\n" + "="*50)
            print("Ð”ÐÐÐÐ«Ð• ÐÐšÐ¢Ð£ÐÐ›Ð¬ÐÐ«")
            print("="*50)
            print("ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð½Ðµ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¹ Ñ„Ð°Ð¹Ð»:")
            print(f"- {self.csv_file}")
            print(f"- ÐœÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ: {self.metadata_file}")
            return
        
        print("\n" + "="*50)
        print("ÐÐÐ§Ð˜ÐÐÐ•Ðœ ÐžÐ‘ÐÐžÐ’Ð›Ð•ÐÐ˜Ð• Ð”ÐÐÐÐ«Ð¥")
        print("="*50)
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´Ð°Ñ‚Ñƒ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ
        last_updated = self.get_last_updated_from_api()
        
        # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ð²ÑÐµ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹
        all_data, all_avatars = self.parse_all_pages()
        
        if all_data:
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² CSV
            self.save_to_csv(all_data, self.csv_file)
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð°Ð²Ð°Ñ‚Ð°Ñ€Ð¾Ð² Ð² Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ñ„Ð°Ð¹Ð»
            self.save_avatars_to_csv(all_avatars, self.avatars_file)
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
            if last_updated:
                self.save_metadata_after_parsing(last_updated, len(all_data))
        else:
            print("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ")

def main():
    """Ð¢Ð¾Ñ‡ÐºÐ° Ð²Ñ…Ð¾Ð´Ð° Ð² Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñƒ"""
    import sys
    
    # Check command line arguments
    tournament_type = "leagues"  # default
    if len(sys.argv) > 1:
        tournament_type = sys.argv[1].lower()
        if tournament_type not in ["leagues", "signals"]:
            print("Error: tournament_type must be 'leagues' or 'signals'")
            print("Usage: python xeet_leaderboard_parser.py [leagues|signals]")
            sys.exit(1)
    
    print(f"Starting parser for {tournament_type.upper()} tournament...")
    parser = XeetLeaderboardParser(tournament_type)
    parser.run()

if __name__ == "__main__":
    main()
